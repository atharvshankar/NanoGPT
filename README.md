
Experimented with the NanoGPT model trained on 3 different datasets — **Shakespeare**, **Wikipedia**, and **Mathematics** — using character-level tokenization.  

The project explores:  
- Investigating token probabilities during text generation.  
- Comparing model performance before and after fine-tuning.  
- Implementing a lightweight version of **GradCAM for language models** to analyze which tokens most influence the probability of generating a target token.  
