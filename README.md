This repository contains my implementation for **Lab 1 of CSE519: Data Science Fundamentals** at Stony Brook University.  
I trained three separate NanoGPT models on different datasets — **Shakespeare**, **Wikipedia**, and **Mathematics** — using character-level tokenization.  

The project explores:  
- Investigating token probabilities during text generation.  
- Comparing model performance before and after fine-tuning.  
- Implementing a lightweight version of **GradCAM for language models** to analyze which tokens most influence the probability of generating a target token.  
